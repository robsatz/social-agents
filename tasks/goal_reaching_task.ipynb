{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Task(control.Task):\n",
    "  \"\"\"Base class for tasks in the Control Suite.\n",
    "\n",
    "  Actions are mapped directly to the states of MuJoCo actuators: each element of\n",
    "  the action array is used to set the control input for a single actuator. The\n",
    "  ordering of the actuators is the same as in the corresponding MJCF XML file.\n",
    "\n",
    "  Attributes:\n",
    "    random: A `numpy.random.RandomState` instance. This should be used to\n",
    "      generate all random variables associated with the task, such as random\n",
    "      starting states, observation noise* etc.\n",
    "\n",
    "  *If sensor noise is enabled in the MuJoCo model then this will be generated\n",
    "  using MuJoCo's internal RNG, which has its own independent state.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, random=None):\n",
    "    \"\"\"Initializes a new continuous control task.\n",
    "\n",
    "    Args:\n",
    "      random: Optional, either a `numpy.random.RandomState` instance, an integer\n",
    "        seed for creating a new `RandomState`, or None to select a seed\n",
    "        automatically (default).\n",
    "    \"\"\"\n",
    "    if not isinstance(random, np.random.RandomState):\n",
    "      random = np.random.RandomState(random)\n",
    "    self._random = random\n",
    "    self._visualize_reward = False\n",
    "\n",
    "  @property\n",
    "  def random(self):\n",
    "    \"\"\"Task-specific `numpy.random.RandomState` instance.\"\"\"\n",
    "    return self._random\n",
    "\n",
    "  def action_spec(self, physics):\n",
    "    \"\"\"Returns a `BoundedArraySpec` matching the `physics` actuators.\"\"\"\n",
    "    return mujoco.action_spec(physics)\n",
    "\n",
    "  def initialize_episode(self, physics):\n",
    "    \"\"\"Resets geom colors to their defaults after starting a new episode.\n",
    "\n",
    "    Subclasses of `base.Task` must delegate to this method after performing\n",
    "    their own initialization.\n",
    "\n",
    "    Args:\n",
    "      physics: An instance of `mujoco.Physics`.\n",
    "    \"\"\"\n",
    "    self.after_step(physics)\n",
    "\n",
    "  def before_step(self, action, physics):\n",
    "    \"\"\"Sets the control signal for the actuators to values in `action`.\"\"\"\n",
    "    # Support legacy internal code.\n",
    "    action = getattr(action, \"continuous_actions\", action)\n",
    "    physics.set_control(action)\n",
    "\n",
    "  def after_step(self, physics):\n",
    "    \"\"\"Modifies colors according to the reward.\"\"\"\n",
    "    if self._visualize_reward:\n",
    "      reward = np.clip(self.get_reward(physics), 0.0, 1.0)\n",
    "      _set_reward_colors(physics, reward)\n",
    "\n",
    "  @property\n",
    "  def visualize_reward(self):\n",
    "    return self._visualize_reward\n",
    "\n",
    "  @visualize_reward.setter\n",
    "  def visualize_reward(self, value):\n",
    "    if not isinstance(value, bool):\n",
    "      raise ValueError(\"Expected a boolean, got {}.\".format(type(value)))\n",
    "    self._visualize_reward = value\n",
    "\n",
    "\n",
    "_MATERIALS = [\"self\", \"effector\", \"target\"]\n",
    "_DEFAULT = [name + \"_default\" for name in _MATERIALS]\n",
    "_HIGHLIGHT = [name + \"_highlight\" for name in _MATERIALS]\n",
    "\n",
    "\n",
    "def _set_reward_colors(physics, reward):\n",
    "  \"\"\"Sets the highlight, effector and target colors according to the reward.\"\"\"\n",
    "  assert 0.0 <= reward <= 1.0\n",
    "  colors = physics.named.model.mat_rgba\n",
    "  default = colors[_DEFAULT]\n",
    "  highlight = colors[_HIGHLIGHT]\n",
    "  blend_coef = reward ** 4  # Better color distinction near high rewards.\n",
    "  colors[_MATERIALS] = blend_coef * highlight + (1.0 - blend_coef) * default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Swimmer(base.Task):\n",
    "  \"\"\"A swimmer `Task` to reach the target or just swim.\"\"\"\n",
    "\n",
    "  def __init__(self, random=None):\n",
    "    \"\"\"Initializes an instance of `Swimmer`.\n",
    "\n",
    "    Args:\n",
    "      random: Optional, either a `numpy.random.RandomState` instance, an\n",
    "        integer seed for creating a new `RandomState`, or None to select a seed\n",
    "        automatically (default).\n",
    "    \"\"\"\n",
    "    super().__init__(random=random)\n",
    "\n",
    "  def initialize_episode(self, physics):\n",
    "    \"\"\"Sets the state of the environment at the start of each episode.\n",
    "\n",
    "    Initializes the swimmer orientation to [-pi, pi) and the relative joint\n",
    "    angle of each joint uniformly within its range.\n",
    "\n",
    "    Args:\n",
    "      physics: An instance of `Physics`.\n",
    "    \"\"\"\n",
    "    # Random joint angles:\n",
    "    randomizers.randomize_limited_and_rotational_joints(physics, self.random)\n",
    "\n",
    "    # Random target position.\n",
    "    close_target = self.random.rand() < .2  # Probability of a close target True %\n",
    "    target_box = .3 if close_target else 2  # Set target box.\n",
    "    xpos, ypos = self.random.uniform(-target_box, target_box, size=2)\n",
    "    #target coordinates\n",
    "    physics.named.model.geom_pos['target', 'x'] = xpos \n",
    "    physics.named.model.geom_pos['target', 'y'] = ypos\n",
    "    #match light's with targets position\n",
    "    physics.named.model.light_pos['target_light', 'x'] = xpos\n",
    "    physics.named.model.light_pos['target_light', 'y'] = ypos\n",
    "\n",
    "    super().initialize_episode(physics)\n",
    "\n",
    "  def get_observation(self, physics):\n",
    "    \"\"\"Returns an observation of joint angles, body velocities and target.\"\"\"\n",
    "    obs = collections.OrderedDict()\n",
    "    obs['joints'] = physics.joints()\n",
    "    obs['to_target'] = physics.nose_to_target()\n",
    "    obs['body_velocities'] = physics.body_velocities()\n",
    "    return obs\n",
    "\n",
    "  def get_reward(self, physics):\n",
    "    \"\"\"Returns a smooth reward.\"\"\"\n",
    "    target_size = physics.named.model.geom_size['target', 0]\n",
    "    return rewards.tolerance(physics.nose_to_target_dist(),\n",
    "                             bounds=(0, target_size),\n",
    "                             margin=5*target_size,\n",
    "                             sigmoid='long_tail')\n",
    "  \n",
    "  #punishment\n",
    "  def get_penalty(self, physics, target_size):\t\n",
    "    \"\"\"Returns a punishment.\"\"\"\n",
    "    punishment_threshold = 3 * target_size\n",
    "    nose_to_target_distance = physics.nose_to_target_dist()\n",
    "\n",
    "    if nose_to_target_distance > punishment_threshold:\n",
    "                punishment = -1 * (nose_to_target_distance - punishment_threshold)\n",
    "    else:\n",
    "                punishment = 0\n",
    "\n",
    "    return punishment\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
